% Created 2016-12-11 Sun 19:20
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\author{Yan-Bin Jia}
\date{\textit{<2016-12-13 Tue 12:00>}}
\title{COM S 577 - Final Exam Study Guide}
\hypersetup{
 pdfauthor={Yan-Bin Jia},
 pdftitle={COM S 577 - Final Exam Study Guide},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 24.5.1 (Org mode 9.0.1)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents


\section{Plane Curves}
\label{sec:org8ed67ce}
Curves are differentiable functions. We denote them \(\boldsymbol{\alpha} : \left[a,b\right] \rightarrow \mathbb{R}^{2} \left(\text{or } \mathbb{R}^{3}\right)\).

A curve \(\boldsymbol{\alpha}\left( t \right) = \left( x\left( t \right), y\left( t \right) \right)\) is said to be \textbf{smooth} at \(t = t_{0}\) if its \(k\text{th}\) derivative exists for any integer \(k > 0\).

\begin{align*}
    \boldsymbol{\alpha}^{\left( k \right)}\left( t \right) = \left( x^{\left( k \right)}\left( t \right), y^{\left( k \right)} \left( t \right) \right)
\end{align*}

A curve is \textbf{piecewise smooth} if it has a domain which is the union of a finite number of subintervals of each of which the curve is smooth. A polygon is piecewise smooth for each of its sides.

A curve is a \textbf{closed parametric curve} if \(\boldsymbol{\alpha}\left( a \right) = \boldsymbol{\alpha}\left( b \right)\). A point of \emph{self-crossing} is a point \(\boldsymbol{\alpha}\left( t_{1} \right)\) for which there exists finitely many distinct values \(t_{1}, \ldots, t_{n} \in \left[a,b\right]\) for \(n \geq 2\) which satisfy \(\boldsymbol{\alpha}\left( t_{1} \right) = \ldots = \boldsymbol{\alpha}\left( t_{n} \right)\) and in the case \(n = 2\), \(\left[t_{1},t_{2}\right] \neq \left[a,b\right]\). For example, a circle is closed, but does not have self-crossings. A figure eight is closed and self-crossing.

\subsection{Velocity, Speed, and Arc Length}
\label{sec:orgbfa5c61}
For a curve \(\boldsymbol{\alpha}\left( t \right)\), the \textbf{velocity vector} at \(t\) is \(\boldsymbol{\alpha}'\left( t \right)\). The \textbf{speed} at \(t\) is the length \(\lvert\lvert \boldsymbol{\alpha}'\left( t \right) \lvert\lvert\). This is clear if we see the curve as a particle moving on a path. The parametrization \(\boldsymbol{\alpha}\left( t \right)\) is \textbf{unit-speed} if the \(\lvert\lvert \boldsymbol{\alpha}'\left( t \right) \lvert\lvert = 1\) for all values of \(t\). When the speed vanishes, the curve has a \textbf{cusp}. A curve is \textbf{regular} if the velocity vectors are non-zero for all \(t\). The \textbf{arc length} is defined by:

\begin{align*}
    \int_{a}^{b} \lvert\lvert \boldsymbol{\alpha}'\left( t \right) \lvert\lvert dt
\end{align*}

\subsection{Unit-Speed Reparametrization}
\label{sec:org9b07120}
A regular curve \(\boldsymbol{\alpha}\) has a reparametrization \(\tilde{\boldsymbol{\alpha}}\). Not all unit-speed reparametrizations have an explicit form, but we can use their existence for many computations on curves.

\subsection{Tangent \(\left( T \right)\) and Normal \(\left( N \right)\)}
\label{sec:orge7c8c0e}
If we have a curve \(\boldsymbol{\alpha}\), then at a regular point on the curve, there exists a non-zero \textbf{tangent vector} \(\boldsymbol{\alpha}'\left( t \right) = \left(x'\left( t \right), y'\left( t \right)\right)\). It represents the velocity of the curve at the point. The normal vector is \(\left(-y'\left( t \right), x'\left( t \right)\right)\). This is a rotation of the tangent vector counterclockwise by \(\frac{\pi}{2}\). If the curve was unit-speed, then the tangent and normal are unit vectors.

The cross product is non-zero. In the case of unit-speed curves, \(T \times N = 1\).

\subsection{Curvature \(\left( \kappa \right)\)}
\label{sec:orgac60b70}
The \textbf{curvature} of a curve is defined as:

\begin{align*}
    \kappa = \frac{d\phi}{ds}
\end{align*}

where \(\phi\) is the angle from the \(x\text{-axis}\) to the tangent and \(s\) is the arc length. The \textbf{absolute curvature} is \(\lvert \kappa \lvert\).

Since \(T \cdot T = 1\) and differentiating this equation yields \(T' \cdot T = 0\), we see that the change of \(T\left( s \right)\) is orthogonal to the tangential direction, so it must be in the normal direction. The curvature is defined as the turning of the tangent along the direction of the unit normal. So,

\begin{align*}
    T' = \frac{dT}{ds} = \kappa N
\end{align*}

\subsection{Center of Curvature, Osculating Circle, Radius of Curvature, and Total Curvature}
\label{sec:org16e5469}
When the curvature is positive then the \textbf{center of curvature} lies along the direction of the normal \(N\left( s \right)\) at a distance \(\frac{1}{\kappa}\) from the point \(\boldsymbol{\alpha}\left( s \right)\). When the curvature is negative, then the center of curvature lies in the direction of \(-N\left( s \right)\) at a distance of \(-\frac{1}{\kappa}\). The \textbf{radius of curvature} is \(\frac{1}{\lvert \kappa \lvert}\). Together, these form the \textbf{osculating circle}. This circle approximates the curve locally up to the second order. The \textbf{total curvature} over a closed interval \(\left[a,b\right]\) measures the rotation of the tangent as \(s\) changes from \(a\) to \(b\).

\begin{align*}
    \Phi\left(a,b\right) &= \int_{a}^{b} \kappa ds \\
    &= \int_{a}^{b} \frac{d\phi}{ds}ds \\
    &= \int_{a}^{b} d\phi \\
    &= \phi\left(b\right) - \phi\left(a\right)
\end{align*}

\subsection{Inflection and Vertex}
\label{sec:orgb441203}
A point \(s\) on the curve \(\boldsymbol{\alpha}\) is a \textbf{simple inflection}, or an \textbf{inflection}, if \(\kappa\left( s \right) = 0\), but \(\kappa'\left( s \right) \neq 0\). Intuitively, this is a point where the curve moves from one side of the tangent to the other or, for simple closed curves, when the curve switches from convex to concave or vice versa.

A point where the curvature and first \(j-1\) order derivatives are zero but the \(j\text{th}\) derivative is non-zero is called an \textbf{inflection point of order \(j\)}. A second order inflection point is also called a point of \textbf{simple undulation} which will maintain concavity or convexity in the neighborhood on a simple closed curve.

A \textbf{simple vertex}, or simply \textbf{vertex}, is a local minimum or maximum when \(\kappa' = 0\) but \(\kappa'' \neq 0\).

\subsection{Arbitrary Speed Curves}
\label{sec:orgbda5c17}
If \(\boldsymbol{\alpha}\) is not necessarily unit-speed, then we obtain the tangent:

\begin{align*}
    T = \frac{\boldsymbol{\alpha}'}{\lvert \lvert \boldsymbol{\alpha}' \lvert \lvert}
\end{align*}

If we let \(v = \frac{\boldsymbol{\alpha}'}{\lvert \lvert \boldsymbol{\alpha}' \lvert \lvert}\), then we have that:

\begin{align*}
    T' = \kappa v N
\end{align*}

Intuitively we are simply normalizing the vectors by the speed. We also note that:

\begin{align*}
    \kappa = \frac{\boldsymbol{\alpha}' \times \boldsymbol{\alpha}''}{\lvert \lvert \boldsymbol{\alpha}' \lvert \lvert^{3}}
\end{align*}

We also find that \textbf{total curvature} is:

\begin{align*}
    \Phi\left(a,b\right) = \int_{a}^{b} \kappa\left(t\right)\lvert\lvert\boldsymbol{\alpha}'\left(t\right)\lvert\lvert dt
\end{align*}

\subsection{Simple Closed Curves}
\label{sec:orgb93326c}
We can say a \textbf{simple closed curve} is one that joins up the beginning and end.

\begin{center}
    \begin{tabular}{ c c c }
        $\boldsymbol{\alpha}\left(t\right) = \boldsymbol{\alpha}\left(u\right)$ & if and only if & $t - u = ka$ for some integer $k$
    \end{tabular}
\end{center}

The \textbf{area} is defined as:

\begin{align*}
    A\left(a\right) &= \int \int_{int\left(\boldsymbol{\alpha}\right)} dxdy \\
    &= \frac{1}{2} \int_{0}^{a} \left(xy' - yx'\right)dt
\end{align*}

Then, we find that if we have a simple closed curve with length \(l\) and area \(A\),

\begin{align*}
    A \leq \frac{1}{4\pi}l^{2}
\end{align*}

\section{Space Curves}
\label{sec:org3ad4fa8}
A plane curve is determined by curvature. Space curves are not. A circle on the \(xy-\text{plane}\) and a circular helix have unit curvature everywhere, but can not be transformed to one another. Instead, we measure space curves with curvature and torsion.

If we have a unit-speed space curve \(\gamma\left(s\right)\) in \(\mathbb{R}^{3}\), we can denote the unit tangent vector as \(T = \gamma'\left(s\right)\). We have the real-valued \textbf{curvature function} \(kappa\left(s\right) = \lvert\lvert T'\left(s\right) \lvert\lvert \geq 0\).

\subsection{Principal Normal}
\label{sec:org95b6913}
The principal normal of a space curve \(\gamma\left( s \right)\) is the vector:

\begin{align*}
    N\left( s \right) = \frac{1}{\kappa\left(s\right)}T'\left(s\right)
\end{align*}

Since \(\lvert\lvert T'\left(s\right) \lvert\lvert = \kappa\), \(N\) is a unit vector. Similarly, \(T' \cdot T = 0\) so T and N are orthogonal to each other.

\subsection{Binormal}
\label{sec:org29d1b77}
The \textbf{binormal} is:

\begin{align*}
    B = T \times N
\end{align*}

Then, \(\left\lbrace T\left(s\right), N\left(s\right), B\left(s\right) \right\rbrace\) form an \textbf{orthonormal basis} of \(\mathbb{R}^{3}\) where each is orthogonal to the other two. They form the \textbf{Frenet frame}.

\subsection{Torsion \(\left( \tau \right)\)}
\label{sec:orgd8e45c9}
We define the \textbf{torsion} as:

\begin{align*}
    B' = -\tau N
\end{align*}

We can see that:

\begin{align*}
    \frac{d\theta}{ds} B'\left(s\right) \cdot N\left(s\right) = -\tau
\end{align*}

\(-\tau\) represents the rate of binormal rotation.

\subsection{Frenet Formulas}
\label{sec:org8102e83}
Given a unit-speed curve with positive curvature and some torsion,

\begin{center}
    \begin{tabular}{ c c c c c }
        $T'$ & $=$ & & $\kappa N$ & \\
        $N'$ & $=$ & $-\kappa T$ & & $+\tau B$ \\
        $B'$ & $=$ & & $-\tau N$ &
    \end{tabular}
\end{center}

Remember the matrix:

\begin{align*}
    \begin{pmatrix}
        0 & \kappa & 0 \\
        -\kappa & 0 & \tau \\
        0 & -\tau & 0
    \end{pmatrix}
\end{align*}

If we multiply by the column vector \(\left( T, N, B \right)^{T}\), then we can get the Frenet formulas.

The \textbf{Frenet frame} consists of the \textbf{curvature function}, the \textbf{torsion function}, the \textbf{unit tangent}, the \textbf{principal normal}, and the \textbf{binormal}. In cases of arbitrary-speed curves, we define:

\begin{align*}
    \kappa &= \tilde{\kappa}\left(s\right)
    \tau &= \tilde{\tau}\left(s\right)
    T &= \tilde{T}\left(s\right)
    N &= \tilde{N}\left(s\right)
    B &= \tilde{B}\left(s\right)
\end{align*}

This is true because the Frenet frame is \textbf{independent} of the parametrization. For example for \(s\) being an arc-length function of the curve, \(\boldsymbol{\alpha}\left(t\right) = \boldsymbol{\alpha}\left(s\left(t\right)\right)\). Only the \textbf{derivatives are affected by parametrization} where we simply use the speed \(v\) of the curve to correct the values of \(T'\), \(N'\), and \(B'\).

\begin{center}
    \begin{tabular}{ c c c c c }
        $T'$ & $=$ & & $\kappa vN$ & \\
        $N'$ & $=$ & $-\kappa vT$ & & $+\tau vB$ \\
        $B'$ & $=$ & & $-\tau vN$ &
    \end{tabular}
\end{center}

Intuitively, a unit-speed curve has speed \(v=1\), so the Frenet formulas are as defined earlier.

\subsubsection{Computing the Frenet Apparatus}
\label{sec:org1f90276}
\textbf{Theorem 3} \emph{Let \(\boldsymbol{\alpha}\) be a regular curve in \(\mathbb{R}^{3}\), and \(\boldsymbol{\alpha}' \times \boldsymbol{\alpha}'' \neq 0\). Then,}

\begin{align*}
T &= \frac{\boldsymbol{\alpha}'}{\lvert\lvert \boldsymbol{\alpha}' \lvert\lvert} \\
B &= \frac{\boldsymbol{\alpha}' \times \boldsymbol{\alpha}''}{\lvert\lvert \boldsymbol{\alpha}' \times \boldsymbol{\alpha}''\lvert\lvert} \\
N &= B \times T \\
\kappa &= \frac{\lvert\lvert \boldsymbol{\alpha}' \times \boldsymbol{\alpha}'' \lvert\lvert}{\lvert\lvert \boldsymbol{\alpha}' \lvert\lvert^{3}} \\
\tau &= \frac{\left(\boldsymbol{\alpha}' \times \boldsymbol{\alpha}''\right) \cdot \boldsymbol{\alpha}'''}{\lvert\lvert \boldsymbol{\alpha}' \times \boldsymbol{\alpha}'' \lvert\lvert^{2}}
\end{align*}

\subsection{Approximation}
\label{sec:orgb142e85}
Consider the unit-speed curve \(\boldsymbol{\beta} = \left( \beta_{1}, \beta_{2}, \beta_{3} \right)\) near the point \(\boldsymbol{\beta}\left(0\right)\). For a small \(s\), each coordinate \(\beta_{i}\left(s\right)\) is closely approximated by the Taylor series. So,

\begin{align*}
    \boldsymbol{\beta}\left(s\right) \approx \boldsymbol{\beta}\left(0\right) + s\boldsymbol{\beta}'\left(0\right) + \frac{s^{2}}{2}\boldsymbol{\beta}''\left(0\right) + \frac{s^{3}}{6}\boldsymbol{\beta}'''\left(0\right)
\end{align*}

However, we know the first and second derivative of \(\boldsymbol{\beta}\) at \(s = 0\) are:

\begin{align*}
    \boldsymbol{\beta}'\left(0\right) = T_{0} \\
    \boldsymbol{\beta}''\left(0\right) = \kappa_{0}N_{0}
\end{align*}

Where subscript is evaluation at \(s = 0\). Then, the third derivative is defined as:

\begin{align*}
    \boldsymbol{\beta}'''\left(0\right) = -\kappa_{0}^{2}T_{0} + \frac{d\kappa}{ds}\left(0\right)N_{0} + \kappa_{0}\tau_{0}B_{0}
\end{align*}

So, the approximation becomes:

\begin{align*}
    \boldsymbol{\beta}\left(s\right) \approx \boldsymbol{\beta}\left(0\right) + sT_{0} + \kappa_{0}\frac{s^{2}}{2}N_{0} + \kappa_{0}\tau_{0}\frac{s^{3}}{6}B_{0}
\end{align*}

This is the \textbf{Frenet approximation of \(\boldsymbol{\beta}\) near \(s = 0\)}. We can get an approximation at some \(s_{0}\) by replacing \(0\) with \(s_{0}\) and \(s\) with \(s - s_{0}\).

\subsection{Osculating Plane}
\label{sec:org88c325e}
Using the \textbf{Frenet approximation}, we can get the best linear or quadratic approximation of the curve. This approximation lies in the plane through the approximated point orthogonal to the binormal. This is called the \textbf{osculating plane of \(\boldsymbol{\beta}\) near \(s_{0}\)}.

\subsection{Spherical Image}
\label{sec:org425338a}
Given a unit-speed curve \(\boldsymbol{\beta}\), then the curve \(\boldsymbol{\sigma}\left(s\right) = T\left(s\right) = \boldsymbol{\beta}'\left(s\right)\) is called the \textbf{spherical image} of \(\boldsymbol{\beta}\).

\section{Algebraic Curves}
\label{sec:orgdd5d66c}
\subsection{Singular Points}
\label{sec:orgc001c49}
A \textbf{singular point} is at a point where the gradient vanishes. We can not simply solve for \(\nabla f = 0\) though. We have to also find a simultaneous solution for \(f = 0\). These are singularities of the polynomial that lie on the curve.
\subsection{Local Parametrization}
\label{sec:orgd08a7a6}
Algebraic curves can be parametrized locally near non-singular points. They can be parametrized by \(x\), \(y\) or both. A \textbf{local parametrization} of an algebraic curve near a non-singular point \(\left( a,b \right)\) is a parametrization \(J \rightarrow \mathbb{R}^{2}\) of a piece of the curve including the point \(\left( a,b \right)\).

\subsection{Curvature}
\label{sec:org3182c96}
In general, algebraic curves cannot be parametrized in a simple way. So, we use a different method of finding the curvature using the \textbf{Hessian matrix} of a polynomial.

\begin{align*}
    H = \begin{pmatrix} f_{xx} & f_{xy} \\ f_{xy} & f_{yy} \end{pmatrix}
\end{align*}

Then, the curvature is defined as:

\begin{align*}
    \kappa = \frac{f_{y}^{2}f_{xx}-2f_{x}f_{y}f_{xy}+f_{x}^{2}f_{yy}}{\left(f_{x}^{2} + f_{y}^{2}\right)^{3/2}}
\end{align*}

\subsection{Inflection Points}
\label{sec:org704d27d}
Algebraic curves have \textbf{inflection points} at non-singular points \(\left(a,b\right)\) if and only if

\begin{align*}
    f_{y}^{2}f_{xx}-2f_{x}f_{y}f_{xy}+f_{x}^{2}f_{yy} = 0
\end{align*}

at the point, changes signs as \(\left(x,y\right)\) moves through the point.

So, to find all inflection points, we solve:

\begin{align*}
    f\left(x,y\right) &= 0 \\
    f_{y}^{2}f_{xx}-2f_{x}f_{y}f_{xy}+f_{x}^{2}f_{yy} &= 0
\end{align*}

\section{Surfaces}
\label{sec:org6ba611f}
\subsection{Surface Patch}
\label{sec:org3d5d0bc}
A \textbf{surface} is viewed as a collection of \textbf{homeomorphisms} \(\boldsymbol{\sigma} : U \rightarrow S \cup W\) called \textbf{surface patches} where \(u\) is a . This collection is called the \textbf{atlas} of the surface. Every point of the surface is in at least one surface patch.

A map is smooth if each component of the map has continuous partial derivatives of all orders. A surface patch is \textbf{regular} if it is smooth and \(\boldsymbol{\sigma}_{u} \times \boldsymbol{\sigma}_{v} \neq 0\) at every point of \(U\). A \textbf{smooth surface} is one whose atlas consists of regular surface patches.

\subsection{Orientable Surface}
\label{sec:org9d13354}
We define the \textbf{tangent space} at a point \(\boldsymbol{p}\) of a surface to consist of the tangent vectors of all curves on the surface that pass through \(\boldsymbol{p}\). It is uniquely defined by the \textbf{unit normal} to the surface at the point, which is perpendicular to the tangent space, also known as the \textbf{tangent plane}.

\begin{align*}
    \boldsymbol{n}_{\sigma} = \frac{\boldsymbol{\sigma}_{u} \times \boldsymbol{\sigma}_{v}}{\lvert\lvert \boldsymbol{\sigma}_{u} \times \boldsymbol{\sigma}_{v} \lvert\lvert}
\end{align*}

This is called the \textbf{standard unit normal}. A surface is \textbf{orientable} if there is a canonical choice for the unit normal at every point, obtained by taking the standard unit normal of each surface patch in the atlas.

\subsection{Surface Curves}
\label{sec:org65b80d2}
If we have a curve in a surface patch, then we can calculate the arc length as:

\begin{align*}
    s = \int_{t_{0}}^{t} \lvert\lvert \dot{\boldsymbol{\alpha}}\left(w\right) \lvert\lvert dw
\end{align*}

The \textbf{normal curvature} and \textbf{geodesic curvature} of a surface curve \(\boldsymbol{\gamma}\left(t\right)\) are:

\begin{align*}
    \kappa_{n} &= \ddot{\boldsymbol{\gamma}} \cdot \hat{\boldsymbol{n}} \\
    \kappa_{g} &= \ddot{\boldsymbol{\gamma}} \cdot \left(\hat{\boldsymbol{n}} \times \dot{\boldsymbol{\gamma}}\right)
\end{align*}

The \textbf{Gaussian curvature} and \textbf{mean curvature} of a surface patch respectively are:

\begin{align*}
    K =  \kappa_{1}\kappa_{2} &= \frac{LN - M^{2}}{EG - F^{2}} \\
    H = \frac{1}{2}\left(\kappa_{1} + \kappa_{2}\right) &= \frac{1}{2}\frac{LG - 2MF + NE}{EG-F^{2}}
 \end{align*}

where \(\kappa_{1}\) and \(\kappa_{2}\) are the principal curvatures.

The \textbf{Gauss map} is the mapping from a surface patch to the unit sphere at from \(\boldsymbol{p} = \boldsymbol{\sigma}\left(u,v\right)\) to the point \(\hat{\boldsymbol{n}}\left(u,v\right)\).

The \textbf{total Gaussian curvature} of some surface is:

\begin{align*}
    \int \int_{S} KdS
\end{align*}

Which is the integral of the Gaussian curvature \(K\) over \(S\). If the surface is defined over the domain \(\left[a,b\right] \times \left[c,d\right]\), then the total Gaussian curvature is:

\begin{align*}
    \int_{c}^{d} \int_{a}^{b} K\left(u,v\right)\sqrt{EG-F^{2}}dudv
\end{align*}

\subsection{First Fundamental Form}
\label{sec:orgbcfc03b}
\textbf{First fundamental form} of a surface patch represents the principal part of the square of the increment on the surface patch when the parameters \(u\) and \(v\) are increased by \(du\) and \(dv\) respectively.

\begin{center}
    \begin{tabular}{ c c c }
        $E = \boldsymbol{\sigma}_{u} \cdot \boldsymbol{\sigma}_{u}$ & $F = \boldsymbol{\sigma}_{u} \cdot \boldsymbol{\sigma}_{v}$ & $G = \boldsymbol{\sigma}_{v} \cdot \boldsymbol{\sigma}_{v}$
    \end{tabular}
\end{center}

So the first fundamental form is:

\begin{align*}
    ds^{2} = Edu^{2} + 2Fdudv + Gdv^{2}
\end{align*}

Thus, the \textbf{arc length} is:

\begin{align*}
    s &= \int_{t_{0}}^{t} \sqrt{E\dot{u}^{2} + 2F\dot{u}\dot{v} + G\dot{v}^{2}}dt \\
    &= \int_{\alpha} = \sqrt{Edu^{2} + 2Fdudv + Gdv^{2}}
\end{align*}

The \textbf{metric tensor} of the surface is:

\begin{align*}
    \mathcal{F}_{1} = \begin{pmatrix} E & F \\ F & G \end{pmatrix}
\end{align*}

The first fundamental form is invariant to parametrization.

\subsection{Second Fundamental Form}
\label{sec:orgd613cce}
Let the standard unit normal be:

\begin{align*}
    \hat{\boldsymbol{n}} = \frac{\boldsymbol{\sigma}_{u} \times \boldsymbol{\sigma}_{v}}{\lvert\lvert \boldsymbol{\sigma}_{u} \times \boldsymbol{\sigma}_{v} \lvert\lvert} = \frac{\boldsymbol{\sigma}_{u} \times \boldsymbol{\sigma}_{v}}{\sqrt{EG - F^{2}}}
\end{align*}

The \textbf{second fundamental form} has elements:

\begin{center}
    \begin{tabular}{ c c c }
        $L = \boldsymbol{\sigma}_{uu} \cdot \hat{\boldsymbol{\sigma}}$ & $M = \boldsymbol{\sigma}_{uv} \cdot \hat{\boldsymbol{\sigma}}$ & $N = \boldsymbol{\sigma}_{vv} \cdot \hat{\boldsymbol{\sigma}}$
    \end{tabular}
\end{center}

And takes the form:

\begin{align*}
    Ldu^{2} + 2Mdudv + Ndv^{2}
\end{align*}

\subsection{Surface Area}
\label{sec:orgd3a8218}
The surface area is:

\begin{align*}
    \int \int_{U} \sqrt{EG - F^{2}} dudv
\end{align*}

\subsection{Darboux Frame}
\label{sec:org7f24427}
If we look at a unit-speed curve \(\gamma\), then we have a frame called the \textbf{Darboux frame} defined by \(\dot{\boldsymbol{\gamma}}\), \(\hat{\boldsymbol{n}}\), and \(\hat{\boldsymbol{n}} \times \dot{\boldsymbol{\gamma}}\). We let these be \(T\), \(U\), and \(V\) respectively. Then, we have:

\begin{align*}
    \begin{pmatrix} T' \\ U' \\ V' \end{pmatrix} =
    \begin{pmatrix}
        0 & \kappa_{g} & \kappa_{n} \\
        -\kappa_{g} & 0 & \tau_{g} \\
        -\kappa_{n} & -\tau_{g} & 0
    \end{pmatrix}
    \begin{pmatrix} T & V & U \end{pmatrix}
\end{align*}

\section{Geodesics}
\label{sec:org99870b5}
A curve is called a \textbf{geodesic} if the acceleration is either zero or parallel to its unit normal \(\hat{\boldsymbol{n}}\).

\textbf{Proposition 1} \emph{Every line can be reparametrized as a geodesic. Thus, a geodesic \(\boldsymbol{\gamma}\left(t\right)\) on a surface \(S\) has constant speed.}

Thus, unit-speed parametrizations of geodesics are also geodesics.

\textbf{Proposition 2} \emph{A curve on a surface is a geodesic if and only if its geodesic curvature is zero everywhere}

\textbf{Theorem 3} \emph{A curve \(\boldsymbol{\gamma}\) on a surface \(S\) is a geodesic if and only if for any part \(\boldsymbol{\gamma}\left(t\right) = \boldsymbol{\sigma}\left(u\left(t\right),v\left(t\right)\right)\) contained in a surface patch \(\boldsymbol{\sigma}\) of \(S\), the following two equations are satisfied:}

\begin{align*}
    \frac{d}{dt}\left(E\dot{u} + F\dot{v}\right) = \frac{1}{2}\left(E_{u}\dot{u}^{2} + 2F_{u}\dot{u}\dot{v} + G_{u}\dot{v}^{2}\right) \\
    \frac{d}{dt}\left(F\dot{u} + G\dot{v}\right) = \frac{1}{2}\left(E_{v}\dot{u}^{2} + 2F_{v}\dot{u}\dot{v} + G_{v}\dot{v}^{2}\right)
\end{align*}

\emph{where \(Edu^{2} + 2Fdudv + Gdv^{2}\) is the first fundamental form of \(\boldsymbol{\sigma}\).}

These two equations are called the \textbf{geodesic equations}. They are nonlinear and solvable analytically on rare occasions only.

\subsection{Isometry}
\label{sec:org3888377}
An \textbf{isometry} is a \textbf{diffeomorphism} that maps curves from one surface to curves of the same length in another surface. A \textbf{diffeomorphism} is a mapping from one surface to another that is smooth, bijective, and has an inverse map that is also smooth. The necessary conditions for \(\boldsymbol{f}\) to be an isometry is any surface patch \(\boldsymbol{\sigma}_{1}\) of \(S_{1}\) and its image patch of \(\boldsymbol{f} \circ \boldsymbol{\sigma}_{1}\) of \(S_{2}\) have the same first fundamental form.

\textbf{Theorem 5} \emph{An isometry maps the geodesics of \(S_{1}\) to the geodesics of \(S_{2}\).}

\subsection{Shortest Paths}
\label{sec:orgf5e11a0}
\textbf{Theorem 6} \emph{Let \(\boldsymbol{\gamma}\) be a shortest path on a surface \(S\) connecting two points \(\boldsymbol{p}\) and \(\boldsymbol{q}\). Then the part of \(\boldsymbol{\gamma}\) contained in any surface patch \(\boldsymbol{\sigma}\) must be a geodesic.}

The converse is not true. Not all geodesics are shortest paths. Additionally, a shortest path may not exist.

\subsection{Geodesic Coordinates}
\label{sec:org7897077}
We can use geodesics of a surface to construct an atlas for the surface. For any given unit-speed geodesic \(\boldsymbol{\gamma}\left(v\right)\) that passes through a point \(\boldsymbol{p}\) in \(S\) at \(v = 0\), then for any value of \(v\), there exists a unique unit-speed geodesic \(\boldsymbol{\beta}_{v}\left(u\right)\) such that \(\boldsymbol{\beta}_{v}\left(0\right) = \boldsymbol{\gamma}\left(v\right)\) and \(\boldsymbol{\beta}_{v}'\left(0\right) \perp \boldsymbol{\gamma}'\left(v\right)\). We define a patch \(\boldsymbol{\sigma}\left(u,v\right) = \boldsymbol{\beta}_{v}\left(u\right)\) as a \textbf{geodesic patch}, and we call \(u\) and \(v\) \textbf{geodesic coordinates}.

\section{Nonlinear Optimization}
\label{sec:org701559d}
Given functions that depend on one or more independent variable, we want to maximize the function. In general, finding global extremum is very difficult. The popular approaches are: i) find local extrema starting from widely varying values of the independent variables, and then pick the most extreme of these; ii) perturb a local extremum by taking a finite amplitude step away from it, and then see if your routine can get to a better point, or "always" to the same one.

\subsection{Golden Section Search}
\label{sec:org0f07e98}
The \textbf{golden section search} is similar to bisection for root finding. The basic idea is to bracket a minimum and then shrink the bracket. We can not know if we have bracketed a minimum, but if we settle for a local minimum, then we can just bracket any zero of the derivative.

\begin{align*}
    f\left(b\right) < \min \left\lbrace f\left(a\right),f\left(c\right) \right\rbrace
\end{align*}

Now, we choose a point \(x\), say, halfway between \(a\) and \(b\). Then, we if \(f\left(x\right) > f\left(b\right)\), then we bracket \(\left[x,b,c\right]\). Otherwise, we bracket \(\left[a,x,b\right]\). To ensure that the interval \(\left[a,c\right]\) shrinks towards a point, we alternate the bracketing tuples.

\subsection{Gradient and Hessian}
\label{sec:org9cd87e2}
The \textbf{gradient} is defined as:

\begin{align*}
    \nabla f\left(x\right) = \left( \frac{\partial f}{\partial x_{1}}, \ldots, \frac{\partial f}{\partial x_{n}} \right)
\end{align*}

This gives the direction in which the value of \(f\) increases the fastest. The \textbf{Hessian H} of \(f\) is defined as an \(n \times n\) matrix:

\begin{align*}
    H\left(\boldsymbol{x}\right) =
    \begin{pmatrix}
        \frac{\partial^{2}f}{\partial x_{1}^{2}} & \cdots & \frac{\partial^{2}f}{\partial x_{1} \partial x_{n}} \\
        \vdots & \ddots & \vdots \\
        \frac{\partial^{2}f}{\partial x_{n} \partial x_{1}} & \cdots & \frac{\partial^{2}f}{\partial x_{n}^{2}}
    \end{pmatrix}
\end{align*}

If \(\boldsymbol{x}^{*}\) is a \textbf{relative minimum}, then the following conditions hold:

\begin{itemize}
    \item $\nabla f\left(\boldsymbol{x}^{*}\right) = 0$
    \item $\boldsymbol{d}^{T}H\left(\boldsymbol{x}^{*}\right)\boldsymbol{x}$ for every $\boldsymbol{d} \in \mathbb{R}^{n}$
\end{itemize}

The Hessian at a relative minimum is \textbf{symmetric positive semi-definite}, so:

\begin{align*}
    \boldsymbol{x}^{T}H\left(\boldsymbol{x}^{*}\right)\boldsymbol{x} \geq 0 \text{ for any } \boldsymbol{x}.
\end{align*}

If \(\boldsymbol{x}^{*}\) is a \textbf{strict relative minimum}, then the Hessian is \textbf{positive definite}, so:

\begin{align*}
    \boldsymbol{x}^{T}H\left(\boldsymbol{x}^{*}\right)\boldsymbol{x} > 0 \text{ for any } \boldsymbol{x} \neq 0
\end{align*}

\subsection{Convex Functions}
\label{sec:org8760f85}
Taylor's theorem shows that every function looks like a quadratic near a strict local minimum. \textbf{Convex functions} are functions whose local minima are global minima.

\textbf{Proposition 1} /Let \(f \in C^{2}\). Then \(f\) is convex over a convex set \(\Omega\) containing an interior point if and only if the Hessian matrix \(H\) is positive semi-definite in \(\Omega\).

\textbf{Theorem 2} \emph{Let \(f\) be a convex function defined on a convex set \(\Omega\). Then the set \(\Gamma\) where \(f\) achieves its minimum value is convex. Furthermore, any relative minimum is a global minimum.}

\subsection{Steepest Descent}
\label{sec:org06c674e}
In one dimension, we apply the rules:

\begin{align*}
    \text{if } f'\left(x\right) < 0 &\text{ then move to the right;} \\
    \text{if } f'\left(x\right) > 0 &\text{ then move to the left;} \\
    \text{if } f'\left(x\right) = 0 &\text{ then stop.}
\end{align*}

In higher dimensions, we use \(-\nabla f\) to point us toward a minimum. This method is called \textbf{steepest descent} because we are descending at the steepest rate.

\subsection{Jacobian}
\label{sec:org8e73283}
The \(Jacobian\) of a function is the derivative of a vector function \(\boldsymbol{f}\left(\boldsymbol{x}\right) = \left(f_{1}\left(\boldsymbol{x}\right), \ldots, f_{m}\left(\boldsymbol{x}\right)\right)\) with respect to \(\boldsymbol{x}\) which results in an \(m \times n\) matrix.

\subsection{Lagrange Multipliers}
\label{sec:org1c007dc}
To solve constrained problems, we introduce the \textbf{Lagrangian} defined as:

\begin{align*}
    \ell\left(x,y,\lambda\right) = f\left(x,y\right) + \lambda h\left(x,y\right)
\end{align*}

Then, we have:

\begin{align*}
    \nabla \ell = \begin{pmatrix} \frac{\partial f}{\partial x} + \lambda\frac{\partial h}{\partial x} \\ \frac{\partial f}{\partial y} + \lambda\frac{\partial h}{\partial y} \\ h \end{pmatrix}^{T} = \left(\nabla f + \lambda\nabla h, h\right)
\end{align*}

\(\lambda\) is known as the \textbf{Lagrangian multiplier}. We set the gradient of the Lagrangian to zero \(\left(\nabla \ell = 0\right)\) to yield a system of nonlinear equations. Solving these yields the stationary points \(\left(x,y,\lambda\right)\), and thus, the minimum and maximum points.

\textbf{Theorem 3 \(\left(First-Order Necessary Conditions\right)\)} \emph{Let \(\boldsymbol{x}^{*}\) be a local extremum point of \(f\) subject to the constraints \(\boldsymbol{h}\left(\boldsymbol{x}\right) = \boldsymbol{0}\). Assume further that \(\boldsymbol{x}^{*}\) is a regular point of these constraints. Then there is a \(\boldsymbol{\lambda} \in \mathbb{R}^{m}\) such that}

\begin{align*}
    \nabla f\left(\boldsymbol{x}^{*}\right) + \boldsymbol{\lambda}^{T}\nabla h\left(\boldsymbol{x}^{*}\right) = \boldsymbol{0}
\end{align*}

\textbf{Theorem 4 \(\left(Second-Order Necessary Conditions\right)\)} \emph{Suppose that \(\boldsymbol{x}^{*}\) is a local minimum of \(f\) subject to \(\boldsymbol{h}\left(\boldsymbol{x}\right) = \boldsymbol{0}\) and that \(\boldsymbol{x}^{*}\) is a regular point of these constraints. Denote by \(F\) the Hessian of \(f\) and by \(H_{i}\) the Hession of \(h_{i}\) for \(1 \leq i \leq m\). Then there is a \(\boldsymbol{\lambda} \in \mathbb{R}^{m}\) such that}

\begin{align*}
    \nabla f\left(\boldsymbol{x}^{*}\right) + \boldsymbol{\lambda}^{T}\nabla h\left(\boldsymbol{x}^{*}\right) = \boldsymbol{0}
\end{align*}

\emph{The matrix}

\begin{align}
    L\left(\boldsymbol{x}^{*}\right) = F\left(\boldsymbol{x}^{*}\right) = \sum_{i = 1}^{m}\lambda_{i}H_{i}\left(\boldsymbol{x}^{*}\right)
\end{align}

\emph{is positive semi-definite on the tangent space} \(\left\lbrace \boldsymbol{y} \mid \nabla\boldsymbol{h}\left(\boldsymbol{x}^{*}\right)\boldsymbol{y} = \boldsymbol{0}\right\rbrace\).

\textbf{Theorem 5 \(\left(Second-Order Sufficient Conditions\right)\)} \emph{Suppose there is a point \(\boldsymbol{x}^{*}\) satisfying \(\boldsymbol{h}\left(\boldsymbol{x}^{*}\right) = \boldsymbol{0}\), and a \(\boldsymbol{\lambda}\) such that}

\begin{align*}
    \nabla f\left(\boldsymbol{x}^{*}\right) + \boldsymbol{\lambda}^{T}\nabla\boldsymbol{h}\left(\boldsymbol{x}^{*}\right) = \boldsymbol{0}
\end{align*}

\emph{Suppose also that the matrix \(L\left(\boldsymbol{x}^{*}\right)\) defined in \(\left(1\right)\) is positive definite on the tangent space. Then \(\boldsymbol{x}^{*}\) is a strict local minimum of \(f\) subject to \(\boldsymbol{h}\left(\boldsymbol{x}\right) = \boldsymbol{0}\)}

\section{Data Fitting}
\label{sec:orge5505a9}
\subsection{Basis Functions}
\label{sec:org1de5f5e}
The \textbf{basis functions} are the functions that make up the variable terms of \(k\) degree for an algebraic function of the form:

\begin{align*}
    \sum_{i = 0}^{n} a_{i}b_{i}\left(x\right)
\end{align*}

Basis functions are not necessarily, for example, \(1, x, x^{2}\). They are any function that spans the vector space.

\subsection{Least-Squares Fitting}
\label{sec:org41c8be3}
Using a linear combination of the chosen basis functions, we can achieve an approximation of the original function through several norms to measure the error. Most often, we use the \textbf{least-squares fitting} that is defined by the norm:

\begin{align*}
    2\text{-norm: } &\lvert\lvert f - F \lvert\lvert_{2} = \sqrt{\sum_{i=1}^{n} \lvert f_{i} - F\left(x_{i}\right)\rvert^{2}}
\end{align*}

To solve for the least-squares fitting, we can use two methods: coefficient matrices or DVD.

The \textbf{coefficient matrix} method is sets \(\boldsymbol{\Phi}\) to be the matrix containing the values of the \$i\$th basis function in the \$i\$th column of \(n\) size where \(n\) is the number of points to be related. We then take \(\boldsymbol{c} = \left(\boldsymbol{\Phi}^{T}\boldsymbol{\Phi}\right)^{-1}\boldsymbol{\Phi}^{T}\boldsymbol{f}\).

The \textbf{SVD method} solves:

\begin{align*}
    \boldsymbol{\Phi}\boldsymbol{c} = \boldsymbol{f}
\end{align*}

\section{Orthogonal Polynomials}
\label{sec:org5023ec4}
\subsection{Inner Product}
\label{sec:org440cafc}
The \textbf{inner product} is defined as:

\begin{align*}
    \left\langle g,h \right\rangle &= \int_{a}^{b}g\left(x\right)h\left(x\right)w\left(x\right) dx \text{  or} \\
    \left\langle g,h \right\rangle &= \sum_{i=1}^{n}g\left(x_{i}\right)h\left(x_{i}\right)w\left(x_{i}\right)
\end{align*}

where \(w\left(x\right)\) is a positive function, called a \textbf{weighting function}. The second form is only used when we do not have information about \(g\) and \(h\) and all points. Instead, we may have information only at \(n\) discrete points \(x_{1}, \ldots, x_{n}\).

Two functions are \textbf{orthogonal} if their inner product vanishes.

Orthogonal functions help with selecting a good basis of functions \(\mathcal{B}\) and approximating a given function or a set of observed data using the basis functions using:

\begin{align*}
f \cong \sum_{g\in\mathcal{B}} \frac{\left\langle f,g \right\rangle}{\left\langle g,g \right\rangle}g
\end{align*}

\subsection{Least-Squares Approximation}
\label{sec:orgf443127}
Given a function \(f\left(x\right)\) defined on some interval \(\left(a,b\right)\), we want to approximate it by a polynomial \(p\left(x\right)\) of degree at most \(k\). We measure the difference between \(f\) and \(p\) by:

\begin{align*}
    \left\langle f\left(x\right) - p\left(x\right), f\left(x\right) - p\left(x\right)\right\rangle
\end{align*}

We find an orthogonal sequence of polynomials for the chosen inner product. Then,

\begin{align*}
    p\left(x\right) = d_{0}p_{0}\left(x\right) + \ldots + d_{k}p_{k}\left(x\right)
\end{align*}

where

\begin{align*}
    d_{i} = \frac{\left\langle f, p_{i} \right\rangle}{\left\langle p_{i}, p_{i} \right\rangle}
\end{align*}

\section{Fourier Series}
\label{sec:org6f70e3a}
The \textbf{Fourier coefficients} of a \textbf{Fourier series} are defined as:

\begin{align*}
    \hat{f}\left(j\right) &= \left\langle f\left(x\right),e^{ijx} \right\rangle \\
    &= \frac{1}{2\pi}\int_{0}^{2\pi} f\left(x\right)e^{-ijx}dx
\end{align*}

\section{Calculus of Variations}
\label{sec:org564d7d8}
\subsection{Functional}
\label{sec:org358f81f}
A \textbf{functional} assigns a real number to each function in some class. In some sense, it is a function of another function.

\begin{align*}
    J\left[y\right] = \int_{a}^{b} F\left(x, y, y'\right)dx
\end{align*}

\subsection{Variational Derivative}
\label{sec:org70c735f}
For some differentiable functional \(J\left[y\right]\), we can find an extremum at \(y = y^{*}\) only if its variation vanishes:

\begin{align*}
    \partial J\left[h\right] = 0
\end{align*}

for all admissible functions \(h\).

\subsection{Euler's Equation}
\label{sec:orgf951fb7}
\textbf{Euler's equation} is:

\begin{align*}
    F_{y} - \frac{d}{dx}F_{y'} = 0
\end{align*}

\section{Probability}
\label{sec:org7a07b4e}
Probability of one or more events \(A\) given a set of possible outcomes \(S\) is defined by Laplace as:

\begin{align*}
    Pr\left(A\right) = \frac{\vert A \vert}{\vert S \vert}
\end{align*}

We also have conditional probability which is defined as the probability of event A given event B where \(Pr\left(B\right) > 0\).

\begin{align*}
     Pr\left(A \mid B\right) = \frac{Pr\left(A \cap B\right)}{Pr\left(B\right)}
\end{align*}

We call the probability of A the \textbf{a priori} probability. The conditional probability of A given B is called the \textbf{a posteriori} probability.

\subsection{Random Variables}
\label{sec:orgd4412cb}
Random variables are functions from the sample space \(S\) of an experiment to the set of real numbers. It simply assigns a real number to each possible outcome.

Random variables can be either \textbf{discrete} or \textbf{continuous}.

\subsection{Cumulative Distribution Function}
\label{sec:orgcdd5d01}
Given a random variable \(X\), its \textbf{cumulative distribution function (CDF)} is defined as:

 \begin{align*}
     D\left( x \right) = Pr\left(X \leq x\right)
\end{align*}

If we have a \textbf{discrete} probability \(Pr\left( x \right)\), that is the probability that a discrete random variable \(X\) has value \(x\), we have:

\begin{align*}
    D\left( x \right) = \sum_{X \leq x} Pr\left( X \right)
\end{align*}

\begin{enumerate}
\item Properties
\label{sec:org5a5627e}
\begin{align*}
    D\left( x \right) &\in \left[0,1\right] \\
    D\left( -\infty \right) &= 0 \\
    D\left( \infty \right) &= 1 \\
    D\left( a \right) &\leq D\left( b \right)\ \text{if} a < b \\
    Pr\left( a < X \leq b \right) &= D\left( b \right) - D\left( a \right)
\end{align*}
\end{enumerate}

\subsection{Probability Density Function}
\label{sec:org5f52c4a}
The \textbf{probability density function (PDF)} \(P\left( x \right)\) of a continuous random variable is defined as the derivative of the cumulative distribution function \(D\left( x \right)\).

\begin{align*}
    P\left( x \right) &= \frac{d}{dx}D\left( x \right) \\
    D\left( x \right) &= \int_{-\infty}^{x} P\left( \xi \right) d\xi
\end{align*}

\begin{enumerate}
\item Properties
\label{sec:orgb64dd9e}
 \begin{align*}
     P\left( x \right) &\geq 0 \\
     \int_{-\infty}^{\infty} P\left( x \right)dx &= 1 \\
     D\left( a < x \leq b \right) &= \int_{a}^{b} P\left( x \right)dx
\end{align*}

A \textbf{uniform distribution} has constant PDF. The PDF and CDF on the interval \(\left[ a,b \right]\) are:

\begin{align*}
    P\left( x \right) &= \begin{cases}
                             0 &\text{for } x < a, \\
                             \frac{1}{b - a} &\text{for } a \leq x \leq b, \\
                             0 &\text{for } x > b;
                         \end{cases} \\
    D\left( x \right) &= \begin{cases}
                             0 &\text{for } x < a, \\
                             \frac{x - a}{b - a} &\text{for } a \leq x \leq b, \\
                             1 &\text{for } x > b;
                         \end{cases}
\end{align*}
\end{enumerate}

\subsection{Generating Continuous Distributions}
\label{sec:org238f474}
Built-in random functions like C++'s \texttt{rand} generate uniformly distributed pseudo-random numbers. If we want to generate uniform random points for a different distribution, say for a circle of radius \(\rho\), then we can do so using CDF.

\(D\left( x \right)\) increases monotonically from zero to one. If we suppose \(D\left( x \right)\) is continuous and strictly increasing, then there exists an inverse \(D^{-1}\left( y \right)\) such that, for \(0 < y < 1\),

\begin{center}
    \begin{tabular}{ c c c }
        $y = D\left( x \right)$ & if and only if & $x = D^{-1}\left( y \right)$
    \end{tabular}
\end{center}

We can compute a random variable \(X\) with distribution \(D\left( x \right)\) by setting:

\begin{align*}
    X = D^{-1}\left( Y \right)
\end{align*}

where \(Y\) is a random variable with uniform distribution over \(\left[0, 1\right]\). The reasoning is:

\begin{align*}
    Pr\left( X \leq x \right) &= Pr\left( D^{-1}\left(Y\right) \leq x \right) \\
    &= Pr\left(Y \leq D\left( x \right)\right) \\
    &= D\left( x \right)
\end{align*}

\textbf{Example: Uniformly Distributed Random Points}

Suppose we want a uniform distribution of random points in a rectangle with dimensions \(\left(l,w\right)\). We can do it by using built in pseudo-random functions such as C++'s \texttt{rand} to get a number in the interval \(\left[0,65535\right]\). We can normalize this to the interval \(\left[0,1\right]\). Now, we can plot a point \(\left(X,Y\right)\) where \(X\) and \(Y\) are uniformly distributed within the intervals \(\left[0,l\right]\) and \(\left[0,w\right]\) respectively. Simply multiply the random output by \(l\) and \(w\) after normalizing.

Now, suppose we want a uniform distribution of random points in a \textbf{circle} of radius \(\rho\). If we simply plot a point \(\left(R,\Theta\right)\) where \(R\) and \(\Theta\) are uniformly distributed within the intervals \(\left[0,\rho\right]\) and \(\left[0,2\pi\right]\) respectively. We will find that the distribution is more densely populated near the origin. Hence, we need to find a distribution for \(R\). Consider the CDF:

\begin{align*}
    D\left( r \right) = Pr\left( R \leq r \right) = \frac{\pi r^{2}}{\pi \rho^{2}} = \frac{r^{2}}{\rho^{2}}
\end{align*}

Clearly, \(D\left( r \right) \in \left[0,1\right]\). So, if we let \(s = D\left( r \right)\), we let \(r = \rho\sqrt{s}\). Introduce a random variable \(S\) with uniform distribution over \(\left[0,1\right]\). So, \(R = \rho\sqrt{S}\).

Thus, random points should be generated at \(\rho\sqrt{S}\left(\cos\Theta,\sin\Theta\right)\), where \(S\) and \(\Theta\) are random variables with uniform distributions over \(\left[0,1\right]\) and \(\left[0,2\pi\right]\).

\subsection{Mean \(\left( \mu \right)\)}
\label{sec:orgdfe5ff2}
The \textbf{mean} or \textbf{expected value}, of a random variable \(X\) is its average value over a large number of experiments (say, \(N\) experiments) with outcomes \(x_{i}\) that occurs \(n_{i}\) times for \(i \in \left\lbrace 1, \ldots, m \right\rbrace\).

\begin{align*}
    E\left( X \right) = \frac{1}{N}\sum_{i=1}^{m}x_{i}n_{i}
\end{align*}

\textbf{Example: Infinite Dice Rolls}

Suppose we roll a die an infinite number of times. Each number appears \(\frac{1}{6}\) of the time. So, expected value is:

\begin{align*}
    E\left( X \right) &= \lim_{n \rightarrow \infty} \frac{1}{n}\sum_{i=1}^{6} i \cdot \frac{n}{6} \\
    &= \frac{7}{2}
\end{align*}

If the random variable \(X\) with PDF \(P\left( X \right)\) is continuous, then the expected is:

\begin{align*}
    E\left( X \right) = \int_{-\infty}^{\infty} xP\left( x \right)dx
\end{align*}
t
\subsection{Standard Deviation \(\left( \sigma \right)\)}
\label{sec:org5bcfa7d}
The \textbf{variance} measure the dispersion of the values about the mean. We can compute the variance of a discrete random variable with \(N\) possible values \(x_{1}, \ldots, x_{N}\) and mean \(\mu\):

\begin{align*}
    var\left( X \right) = \sum_{i=1}^{N} Pr\left( x_{i} \right)\left( x_{i} - \mu \right)^{2}
\end{align*}

\(var\left( X \right) = \sigma^{2}\), and we call \(\sigma\) the \textbf{standard deviation}. For a continuous distribution:

\begin{align*}
    var\left( X \right) = \int_{-\infty}^{\infty} P\left( X \right)\left( x - \mu \right)^{2}dx
\end{align*}

So, we can rewrite variance as:

\begin{align*}
    \sigma^{2} &= E\left(\left( X - \mu \right)\left( X - \mu \right)\right) \\
    &= E\left( X^{2} - 2X\mu + \mu^{2} \right) \\
    &= E\left( X^{2} \right) - 2\mu^{2} + \mu^{2} \\
    &= E\left( X^{2} \right) - \mu^{2}
\end{align*}

\subsection{Gaussian Distribution}
\label{sec:org1357433}
A random variable \(X\) with mean \(\mu\) and variance \(\sigma^{2}\) has \textbf{Gaussian distribution} or \textbf{normal distribution} if its probability density function is given by:

\begin{align*}
    P\left( x \right) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\left( x - \mu \right)^{2}/\left(2\sigma^{2}\right)}
\end{align*}

We denote the Gaussian distribution as \(N\left( \mu,\sigma^{2} \right)\).

\subsection{Covariant (Matrix)}
\label{sec:org5ef496f}
Covariance of random variables \(X\) and \(Y\) with means \(\mu_{X}\) and \(\mu_{Y}\) respectively is defined as:

\begin{align*}
    \text{cov}\left(X,Y\right) &= E\left(\left(X-\mu_{X}\right)\left(Y-\mu_{Y}\right)\right) \\
    &= E\left(XY\right) - \mu_{X}\mu{Y}
\end{align*}

\subsection{Correlation (Matrix)}
\label{sec:org4d6f984}
The \textbf{correlation coefficient} of two variables is:

\begin{align*}
    \text{cor}\left(X,Y\right) &= \frac{\text{cov}\left(X,Y\right)}{\sigma_{X}\sigma_{Y}}
\end{align*}
\end{document}
